{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chelombus import DataStreamer\n",
    "from chelombus import FingerprintCalculator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINGERPRINT MODULE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example on how the API works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprints shape: (4, 1024)\n",
      "Fingerprint 1 [0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Define a list of SMILES strings\n",
    "smiles_list = [\"CCO\", \"C1CCCCC1\", \"O=C=O\", \"O=C=O\"]\n",
    "\n",
    "# Define fingerprint parameters\n",
    "params = {'fpSize': 1024, 'radius': 2}\n",
    "\n",
    "# Create an instance of FingerprintCalculator\n",
    "calculator = FingerprintCalculator()\n",
    "\n",
    "# Compute fingerprints for the list of SMILES strings\n",
    "fingerprints = calculator.FingerprintFromSmiles(smiles_list, 'morgan', **params)\n",
    "\n",
    "# Display the shape of the output fingerprint array\n",
    "print(f\"Fingerprints shape: {fingerprints.shape}\")\n",
    "print(\"Fingerprint 1\", fingerprints[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the API for loading fingerprints in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      " Fingerprints calculated: 100,000"
     ]
    }
   ],
   "source": [
    "# Import iterator method\n",
    "ds = DataStreamer()\n",
    "\n",
    "chunksize = 10_000\n",
    "smiles= ds.parse_input(input_path=\"smiles_examples.smi\", chunksize=chunksize, smiles_col=1)\n",
    "print(type(smiles)) # This is only the generator, in order to get each chunk of data we need to iterate\n",
    "\n",
    "count = 0\n",
    "for smiles_chunk in smiles:\n",
    "     count += len(smiles_chunk)\n",
    "     calculator.FingerprintFromSmiles(smiles_chunk, 'morgan', **params)\n",
    "     print(f\"\\r Fingerprints calculated: {count:,}\", end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to save each chunk as a separate file -ideal for large chunks that we could use later- then `save_chunk` from the `helper_functions.py`is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fingerprints calculated: 100,000"
     ]
    }
   ],
   "source": [
    "from chelombus.utils.helper_functions import save_chunk\n",
    "\n",
    "smiles= ds.parse_input(input_path=\"smiles_examples.smi\", chunksize=chunksize, smiles_col=1)\n",
    "\n",
    "count = 0\n",
    "for idx, smiles_chunk in enumerate(smiles):\n",
    "    count += len(smiles_chunk)\n",
    "    fp_chunk = calculator.FingerprintFromSmiles(smiles_chunk, 'morgan', **params)\n",
    "    save_chunk(fp_chunk, output_dir=\"data\", chunk_index=idx, file_format='npy')\n",
    "    print(f\"\\r Fingerprints calculated: {count:,}\", end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1024) 102.4  MB\n"
     ]
    }
   ],
   "source": [
    "# At this step we could load Fingerprints computed in the previous step -likely what you'd do for large datasets\n",
    "# or in case it fits in memory just pass the fingerprint matrix\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"smiles_examples.smi\", sep=\"\\t\", header=None, names=[\"id\", \"smiles\"])\n",
    "smiles = df[\"smiles\"].tolist()\n",
    "fingerprints = calculator.FingerprintFromSmiles(smiles, 'morgan', fpSize=1024, radius=3)\n",
    "\n",
    "print(fingerprints.shape, f\"{fingerprints.nbytes / 1e6}  MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit the PQ encoder using our training data. \n",
    "`K`refers to the number of centroids to be used when running KMeans on each subvector. \n",
    "`m`is the number of subvectors (splits) from our input data. \n",
    "`iterations`is the maximum number of iterations each KMeans is going to do. \n",
    "With higher `K`and `iterations`, higher training times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PQ-encoder: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from chelombus.encoder.encoder import PQEncoder\n",
    "\n",
    "pq_encoder = PQEncoder(k=256, m=4, iterations=10)\n",
    "pq_encoder.fit(fingerprints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check some atributes: \n",
    "\n",
    "`.codewords` are the centroids coordinates gathered from each KMeans run on every subvector. Since we have 4 splits, 256 centroids and the subvectors are of size 1024/4 = 256, then the codebook is shape (4, 256, 256)\n",
    "\n",
    "After the `pq_encoder` is fitted, the encoder has an attribute to account for the training process. If we try to use transform without fitting we would get an Error. So know, we check that the ecoder was in fact trained. \n",
    "\n",
    "If we want to access all the `KMeans`attributes that one would normally get from sklearn, we can do so using the list of KMeans in `pq_trained` and use any attribute you would normally use. Like `.labels_` to check the index of the centroids for each training sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the codebook is:  (4, 256, 256)\n",
      "Is the encoder trained?  True\n",
      "The lables: KMeans(max_iter=10, n_clusters=256) have length: 100000\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the codebook is: \", pq_encoder.codewords.shape)\n",
    "print(\"Is the encoder trained? \", pq_encoder.encoder_is_trained)\n",
    "print(f\"The lables: {pq_encoder.pq_trained[0]} have length: {len(pq_encoder.pq_trained[0].labels_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training process we can create our PQ codes.\n",
    "The PQCodes are going to be of shape `(Number of samples, m)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,000 fingerprints of 1024 dimensions to be transformed into PQ-codes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating PQ-codes: 100%|██████████| 4/4 [00:00<00:00, 88.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming 10,000 fingeprints took 0.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "\n",
    "# First we load and calculate our fingerprints \n",
    "X_test = np.load('data/fingerprints_chunk_00000.npy')\n",
    "print(f\"{X_test.shape[0]:,} fingerprints of {X_test.shape[1]} dimensions to be transformed into PQ-codes\")\n",
    "\n",
    "s = time.time()\n",
    "X_pq_code = pq_encoder.transform(X_test)\n",
    "e = time.time()\n",
    "print(f\"Transforming {X_test.shape[0]:,} fingeprints took {(e-s):.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a PQ-code of 4 digits and each digit can take value {0,255} then the number of theoretical unique PQ-codes we can get is therefore $256^4 = 4,294,967,296$. However we can test that this is much less in reality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33,  91,  53,  83],\n",
       "       [ 33, 110, 134,  83],\n",
       "       [ 91,  91,  90, 119],\n",
       "       ...,\n",
       "       [ 70,  47, 199,  96],\n",
       "       [ 70,  47, 156,  18],\n",
       "       [228,  31, 156, 204]], shape=(10000, 4), dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pq_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 4-dim vectors: 6273\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Count unique rows\n",
    "unique_rows = np.unique(X_pq_code, axis=0)\n",
    "num_unique_vectors = unique_rows.shape[0]\n",
    "print(\"Number of unique 4-dim vectors:\", num_unique_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of transforming the binary fingerprints into PQ-codes is that we are storing (almost) the same information in a much more efficient way. We can check that the amount of memory required to store the same data is 256x times less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input of shape: (10000, 1024) and size of 10,240,000 bytes is now transformed into shape (10000, 4) and size of 40,000 bytes\n",
      "This is 256 times more memory efficient\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original input of shape: {X_test.shape} and size of {X_test.nbytes:,} bytes is now transformed into shape {X_pq_code.shape} and size of {X_pq_code.nbytes:,} bytes\")\n",
    "print(f\"This is {int(X_test.nbytes / X_pq_code.nbytes)} times more memory efficient\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PQKMeans Clustering\n",
    "\n",
    "After encoding data into PQ-codes, we can perform efficient clustering directly on the compressed representation using PQKMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 10000 molecules to 100 clusters\n",
      "Labels shape: (10000,)\n",
      "Sample labels: [47 76 36 36 36 94 94 94 57 94]\n"
     ]
    }
   ],
   "source": [
    "from chelombus import PQKMeans\n",
    "\n",
    "# Create a PQKMeans clusterer\n",
    "# k: number of clusters\n",
    "# iteration: number of k-means iterations\n",
    "n_clusters = 100\n",
    "clusterer = PQKMeans(encoder=pq_encoder, k=n_clusters, iteration=20, verbose=False)\n",
    "\n",
    "# Fit and predict in one step\n",
    "labels = clusterer.fit_predict(X_pq_code)\n",
    "print(f\"Assigned {len(labels)} molecules to {n_clusters} clusters\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Sample labels: {labels[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access cluster centers (as PQ codes) and check cluster distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers shape: (100, 4)\n",
      "\n",
      "Cluster size statistics:\n",
      "  Min cluster size: 1\n",
      "  Max cluster size: 304\n",
      "  Mean cluster size: 102.0\n"
     ]
    }
   ],
   "source": [
    "# Cluster centers are stored as PQ codes\n",
    "print(f\"Cluster centers shape: {clusterer.cluster_centers_.shape}\")\n",
    "\n",
    "# Check cluster distribution\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(f\"\\nCluster size statistics:\")\n",
    "print(f\"  Min cluster size: {counts.min()}\")\n",
    "print(f\"  Max cluster size: {counts.max()}\")\n",
    "print(f\"  Mean cluster size: {counts.mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `fit()` and `predict()` separately, useful when assigning new data to existing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [16 16 36 36 36 10 10 10 18 33]\n"
     ]
    }
   ],
   "source": [
    "# Fit on training data\n",
    "clusterer2 = PQKMeans(encoder=pq_encoder, k=50, iteration=15)\n",
    "clusterer2.fit(X_pq_code)\n",
    "\n",
    "# Predict on new data (using same data for demo)\n",
    "new_labels = clusterer2.predict(X_pq_code)\n",
    "print(f\"Predicted labels: {new_labels[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "Both the PQEncoder and PQKMeans can be saved and loaded for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model predictions match: True\n"
     ]
    }
   ],
   "source": [
    "# Save the trained clusterer\n",
    "clusterer2.save(\"data/pqkmeans_model.joblib\")\n",
    "\n",
    "# Load the model later\n",
    "loaded_clusterer = PQKMeans.load(\"data/pqkmeans_model.joblib\")\n",
    "\n",
    "# Use the loaded model to predict\n",
    "loaded_labels = loaded_clusterer.predict(X_pq_code)\n",
    "print(f\"Loaded model predictions match: {np.array_equal(new_labels, loaded_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster I/O\n",
    "\n",
    "The `cluster_io` module provides utilities to query and export clustered data using DuckDB. This is useful for large-scale datasets stored as chunked parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 parquet chunks\n"
     ]
    }
   ],
   "source": [
    "# First, let's create sample parquet files for demonstration\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "\n",
    "# Create a dataframe with smiles and cluster_id\n",
    "cluster_df = pd.DataFrame({\n",
    "    \"smiles\": smiles[:len(labels)],\n",
    "    \"cluster_id\": labels\n",
    "})\n",
    "\n",
    "# Save as chunked parquet (simulating pipeline output)\n",
    "chunk_size = 5000\n",
    "for i, start in enumerate(range(0, len(cluster_df), chunk_size)):\n",
    "    chunk = cluster_df.iloc[start:start + chunk_size]\n",
    "    chunk.to_parquet(f\"data/results/chunk_{i:05d}.parquet\", index=False)\n",
    "\n",
    "print(f\"Created {len(os.listdir('data/results'))} parquet chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 98\n",
      "Total molecules: 10,000\n",
      "\n",
      "Top 5 largest clusters:\n",
      "    cluster_id  molecule_count\n",
      "11          11             304\n",
      "36          36             281\n",
      "78          79             276\n",
      "90          91             251\n",
      "54          54             250\n"
     ]
    }
   ],
   "source": [
    "from chelombus import get_cluster_stats, query_cluster, sample_from_cluster\n",
    "\n",
    "# Get statistics for all clusters\n",
    "stats = get_cluster_stats(\"data/results/\")\n",
    "print(f\"Number of clusters: {len(stats)}\")\n",
    "print(f\"Total molecules: {stats['molecule_count'].sum():,}\")\n",
    "print(f\"\\nTop 5 largest clusters:\")\n",
    "print(stats.nlargest(5, 'molecule_count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 contains 74 molecules\n",
      "                                              smiles  cluster_id\n",
      "0  CC(C1=NN=C(SCCC2=CC=NC=C2)N1C1=CC=C(F)C=C1)N1C...           0\n",
      "1  CC(C1=NN=C(SCC2CC(=O)N(C)C2)N1C1=CC=C(F)C=C1)N...           0\n",
      "2  CC(C1=NN=C(SCC2=C(Br)N(C)N=C2)N1C1=CC=C(F)C=C1...           0\n",
      "3  CC(C1=NN=C(SC[C@H]2C[C@H]3C=C[C@@H]2O3)N1C1=CC...           0\n",
      "4  CC(C1=NN=C(SCC(=O)C2=C(F)C=CC=C2F)N1C1=CC=C(F)...           0\n"
     ]
    }
   ],
   "source": [
    "# Query all molecules from a specific cluster\n",
    "cluster_id = stats.iloc[0]['cluster_id']  # Get largest cluster\n",
    "cluster_data = query_cluster(\"data/results/\", cluster_id=int(cluster_id))\n",
    "print(f\"Cluster {cluster_id} contains {len(cluster_data)} molecules\")\n",
    "print(cluster_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of 5 molecules from cluster 0:\n",
      "['FC1=CC=C(N2C(CN3CCCCC3)=NN=C2SCC2=CC=NC(Cl)=C2F)C=C1', 'O=C(CSC1=NN=C(CN2CCCCC2)N1C1=CC=CC=C1)C1=CC=CNC1=O', 'CC(C1=NN=C(SCC2=CC=C(C(=O)O)C(F)=C2)N1C1=CC=C(Cl)C=C1)N(C)C', 'COC1=CC(CSC2=NN=C(CN3CCCCC3)N2C2=CC=C(F)C=C2)=NC=N1', 'CC(C)N(C)C[C@H](O)CSC1=NN=C(CN2CCCCC2)N1C1=CC=C(F)C=C1']\n"
     ]
    }
   ],
   "source": [
    "# Get a random sample from a cluster (useful for previewing)\n",
    "sample = sample_from_cluster(\"data/results/\", cluster_id=int(cluster_id), n=5, random_state=42)\n",
    "print(f\"Random sample of 5 molecules from cluster {cluster_id}:\")\n",
    "print(sample['smiles'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 74 molecules to data/cluster_export.parquet\n"
     ]
    }
   ],
   "source": [
    "from chelombus import export_cluster\n",
    "\n",
    "# Export a single cluster to a file\n",
    "count = export_cluster(\n",
    "    \"data/results/\", \n",
    "    cluster_id=int(cluster_id), \n",
    "    output_path=\"data/cluster_export.parquet\"\n",
    ")\n",
    "print(f\"Exported {count} molecules to data/cluster_export.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with TMAP\n",
    "\n",
    "Create interactive tree maps (TMAPs) to visualize molecular clusters. TMAPs use LSH Forest to find nearest neighbors and create a tree-like layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Because TMAP is outdated it is not compatible with Python versions above 3.8. I am working on creating a new version of TMAP so in the meantime if you wish to create the TMAPs you could \n",
    "\n",
    "- a downgrade the version to 3.8\n",
    "\n",
    "- b install tmap in a separate environment and use that one when generating the maps. \n",
    "\n",
    "Sorry for the inconvenience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afloresep/work/Chelombus/venv/lib/python3.12/site-packages/faerun/faerun.py:409: RuntimeWarning: invalid value encountered in divide\n",
      "  data_c[s] = (data_c[s] - min_c[s]) / (max_c[s] - min_c[s])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500\"\n",
       "            src=\"./sample_tmap_TMAP.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x78c0f5b9c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./sample_tmap_TMAP.html' target='_blank'>./sample_tmap_TMAP.html</a><br>"
      ],
      "text/plain": [
       "/home/afloresep/work/Chelombus/examples/sample_tmap_TMAP.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMAP generated: sample_tmap_TMAP.html\n"
     ]
    }
   ],
   "source": [
    "from chelombus.utils.visualization import create_tmap\n",
    "# Create a TMAP for a subset of molecules (for demonstration)\n",
    "# Note: For large datasets, create TMAPs per cluster\n",
    "sample_smiles = smiles[:1000]\n",
    "\n",
    "# This will generate an interactive HTML file\n",
    "create_tmap(\n",
    "    smiles=sample_smiles,\n",
    "    fingerprint='morgan',\n",
    "    tmap_name='sample_tmap'\n",
    ")\n",
    "print(\"TMAP generated: sample_tmap_TMAP.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This tutorial covered the main Chelombus API:\n",
    "\n",
    "1. **Fingerprint Calculation**: Use `FingerprintCalculator` to compute molecular fingerprints\n",
    "2. **Data Streaming**: Use `DataStreamer` to process large files in chunks\n",
    "3. **Product Quantization**: Use `PQEncoder` to compress fingerprints into compact PQ-codes\n",
    "4. **Clustering**: Use `PQKMeans` to cluster PQ-codes efficiently\n",
    "5. **Cluster I/O**: Use `query_cluster`, `export_cluster`, `get_cluster_stats` etc. for querying clustered data\n",
    "6. **Visualization**: Use `create_tmap` to generate interactive visualizations\n",
    "\n",
    "For billion-scale datasets, combine these components in a streaming pipeline:\n",
    "- Stream SMILES in chunks\n",
    "- Compute fingerprints per chunk\n",
    "- Transform to PQ-codes\n",
    "- Cluster with PQKMeans\n",
    "- Export results as parquet\n",
    "- Visualize per-cluster TMAPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
